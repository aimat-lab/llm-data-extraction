
% --------------------------------------
%           Acronyms
% --------------------------------------

\newacronym{MLP}{MLP}{Multi-Layer Perceptron}
\newacronym{LOC}{LOC}{Lines of Code}
\newacronym{ReLU}{ReLU}{Rectified Linier Unit}
\newacronym{SwiGLU}{SwiGLU}{Swish Gated Linear Unit}
\newacronym{ML}{ML}{Machine Learning}
\newacronym{LM}{LM}{Language Model}
\newacronym{LLM}{LLM}{Large Language Model}
\newacronym{NER}{NER}{Named Entity Recognition}
\newacronym{ER}{ER}{Entity Recognition}
\newacronym{MOF}{MOF}{Metal Organic Framework}


\newacronym{TF-IDF}{TF-IDF}{Term Frequency - Inverse Document Frequency}
\newacronym{BM25}{BM25}{Okapi Best-Matching}
\newacronym{BERT}{BERT}{Bidirectional Encoder Representation from Transformers \cite{devlin_bert_2018}}
\newacronym{SOTA}{state-of-the-art}{State of the Art}
\newacronym{NLP}{NLP}{Natural Language Processing}
\newacronym{RoPE}{RoPE}{Rotary Positional Encoding \cite{su_roformer_2022}}
\newacronym{GQA}{GQA}{Grouped Query Attention \cite{ainslie_gqa_2023}}
\newacronym{GPT}{GPT}{Generative Pretrained Transformer}

\newacronym{transformers}{\texttt{transformers}}{\gls{hf} \texttt{transformers} \cite{huggingface_2023}}


% --------------------------------------
%           Terms
% --------------------------------------

\newglossaryentry{causal}{
    name=causal language model,
    description={Causal language modeling is the task of predicting the token following a sequence of tokens. This in contrast to e.g. a \gls{masked} }
}

\newglossaryentry{masked}{
    name=masked language model,
    description={Masked language modeling is the task of predicting the missing (replaced with marker tokens) words in a sentence based on the context provided by the surrounding words. This in contrast to e.g. a \gls{causal}}
}


\newglossaryentry{fscore}{
    name=F-Score,
    description={\textbf{Definition.} $ 2\times \frac{precision\ \cdot\ recall}{precision + recall}$}
}



% --------------------------------------
%           Models
% --------------------------------------



\newglossaryentry{GPT2}{
    name=GPT2,
    description={The second generation \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer \gls{LM} from \gls{OpenAI} \cite{radford_language_2019}}
}

\newglossaryentry{GPT3}{
    name=GPT3,
    description={The third generation \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer \gls{LM} from \gls{OpenAI} \cite{brown_language_2020}}
}

\newglossaryentry{ChatGPT}{
    name=ChatGPT,
    description={A chat interface for \gls{GPT3}.5 or \gls{GPT4} from \gls{OpenAI}, which makes it easier to task the model}
}

\newglossaryentry{GPT4}{
    name=GPT4,
    description={The fourth generation \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer \gls{LM} from \gls{OpenAI} \cite{openai_gpt4_2023}. Currently their most capable model}
}

\newglossaryentry{BLOOM}{
    name=BLOOM,
    description={\textbf{B}igScience \textbf{L}arge \textbf{O}pen-science \textbf{O}pen-access \textbf{M}ultilingual \gls{LM}, a 176 billion parameter open-source \gls{LLM} created through a cooperation between \gls{Google}, \gls{hf} and various smaller organisations \cite{workshop_bloom_2022}.}
}

\newglossaryentry{OPT}{
    name=OPT,
    description={Open Pretrained Transformer, a 175 billion parameter open-source \gls{LM} from \gls{meta} research \cite{zhang_opt_2022}}
}

\newglossaryentry{PaLM}{
    name=PaLM,
    description={extremely capable \gls{LLM} from \gls{Google} \cite{chowdhery_palm_2022}}
}

\newglossaryentry{llama}{
    name=LLaMa,
    description={A \gls{LLM} from \gls{meta}. See \subref{llama} for details}
}

\newglossaryentry{alpaca}{
    name=Stanford Alpaca,
    description={A \gls{LLM} based on \gls{llama}. See \subref{alpaca} for details}
}

\newglossaryentry{vicuna}{
    name=Vicuna,
    description={One of the \glspl{LLM} used.  See \subref{vicuna} for details}
}

\newglossaryentry{llama2}{
    name=LLaMa 2,
    description={One of the \glspl{LLM} used. See \subref{llama2} for details}
}

\newglossaryentry{falcon}{
    name=Falcon,
    description={One of the \glspl{LLM} used. See \subref{falcon} for details}
}

% --------------------------------------
%           Organizations
% --------------------------------------

\newglossaryentry{meta}{
    name=Meta,
    description={Previously known as Facebook, Meta is a deep learning powerhose and regularly open-sources new \gls{SOTA} machine learning models.
    }
}

\newglossaryentry{OpenAI}{
    name=OpenAI,
    description={American AI company, trailbrlazing at the frontier of scaling deep learning architectures and algorithmic breakthroughs for doing so. Their currently most well-known models are the \gls{GPT} family of models, particularly \gls{GPT2}, \gls{GPT3} and \gls{GPT4}}
}

\newglossaryentry{Google}{
    name=Google,
    description={Search engine giant turned AI powerhouse, well known for training supersized machine learning models with unrealistic hardware requirements for just about anyone else}
}

\newglossaryentry{hf}{
    name=HuggingFace,
    description={American deep learning ecosystem startup, having a well established framework to interface and abstract between similar models}
}

\newglossaryentry{tii}{
    name=Technology Innovation Institute,
    description={Abu Dhabi-based machine learning research institute}
}

\newglossaryentry{lmsys}{
    name=LMSYS,
    description={The Large Model Systems Organization eponymously develops large models and systems that are open, accessible, and scalable}
}

