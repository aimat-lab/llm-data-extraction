\chapter{Approach}\label{chap:approach}
Here, we describe some of our approach to using \glspl{LLM}, decisions made, lessons learned, and more.

Specifically, we put little effort inprompt engineering, with the reason being that it should have little impact for fine-tuned models.



See \subref{list} for a list of the models used, \subref{criteria} for selection criteria for the models, and the respective Subsection for more details on each model, which is als linked in their glossary entry.

All source code for this work can be found at \url{https://github.com/fkarg/mthesis}.
A release will tag the state at the time of submission.

\section{Prompts}\label{sec:prompts}
\glspl{LLM} are capable of outputting a wide variety of text, and solving a broad array of different tasks.
The way to get them to solve the task as requested is by \em{prompting} the model with a certain input.
This is particularly emintent in instruct-based models (See \subref{instruct} for more details on instruction-based finetuning).

\subsection{Prompt Engineering}\label{sub:engineering}
Answers, even to the same prompts, across models and even from the same model, can vary substantially \cite{chen_how_2023}.
Thus, a short-lived 'discipline' referred to as Prompt Engineering emerged, which attempted to scientifically approach how to write prompts to get the best results, out of either specific or all models.
% This is particularly relevant for applications where potentially hostile users can directly or indirectly prompt a model, and thus 'Prompt Injection Attacks' where born \cite{greshake_more_2023}.
As was found out, this is also a task that could be perfectly done by a \gls{LLM} \cite{zhou_large_2022}.

\subsection{Prompt Guidelines}\label{sub:guidelines}
A few general guidelines for prompts empirically emerged (mostly through people sharing results on twitter):
\begin{itemize}
    \item Guidance for everything structure-based \cite{guidance_2023}
    \item Chain-Of-Thought for reasoning \cite{wei_chainofthought_2022}
    \item Reflexion for even bigger models \cite{shinn_reflexion_2023}
\end{itemize}

\subsection{Prompts Used}\label{sub:prompts}

\code{prompt.py}{prompt1}{Example of prompt structure used}



\section{Supervised Fine Tuning}\label{sec:sft}
short intro to fine-tuning, why it is important. \secref{training} and \subref{finetune}



curiousity with \verb`input_ids` and \verb`labels`? resulted in broken model, probably learned that it's 'finished', only outputting EOS tokens. Not sure if doing this otherwise would actually change anything though

Attempts at mask manipulation: not possible with causalLMs (they are all of this type)

SFT: only published about 1months before deadline, a lot of magic that isn't documented properly, at all. Couldn't get it to run, gave up due to time limit.

\section{Comparison}
criteria for comparing the outputs with labels

coneversion between units (not super straightforward), a lot of unit mistakes

solvents and additives: getting cid and
