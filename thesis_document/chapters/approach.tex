\chapter{Approach}\label{chap:approach}
This chapter describes all relevant decisions of the approach taken, as well as their reasons.

First, \secref{impl} broadly describes the implementation, as well as libraries and frameworks used.
In \secref{models} descriptions and references to the various models considered for this work can be found.
\secref{prompts} discusses how the models where prompted in detail, and why little effort and experimentation was done with different prompts.
The dataset used, and how it was processed is described in \secref{data}, before it is depicted how the comparison for equality was done in \secref{equality}.


\section{Implementation}\label{sec:impl}
All source code for this work can be found at \url{https://github.com/fkarg/mthesis}, and a tag will mark the state at the time of submission.
\todo{publish on day of submission}

It became appearent during literature research that a comparison between different models would be valuable, and that additional models can be expected to be released in time to be included in this work.
Thus, almost all code ought to be model-agnostic.
The \acrlong{transformers} library was chosen due to being a well-established framework providing abstractions to load, manipulate and train any deep learning architecture in a standardized format.
Additionally, all open-access \glspl{LLM} are available directly through the \gls{hf} portal.

\paragraph{Dependencies}
Most other dependencies used are either straightforward (\texttt{torch}, \texttt{einops}, \texttt{accelerate}, \texttt{bitsandbytes} to get the models to run) or common ecosystem choices (e.g. \texttt{typer} and \texttt{rich} for the cli interface; \texttt{pubchempy} to resolve and convert chemical compounds; etc).

\paragraph{Modularity}
Proportionally speaking, the main module (with 36\%), dataloader (with 23\%), and unit conversion module (with 11\%) have the highest \gls{LOC} counts. Everything else is split in six more supporting modules.


\input{chapters/models}


\section{Prompts}\label{sec:prompts}
\glspl{LLM} are capable of very generic tasks, based on the input they are asked to respond to.
\textit{Prompting} a model with a certain input gives more fine-grained control over the output and can provide additional structure, information, and suggestions for solving a specified task.
This is particularly eminent in models fine-tuned for instruction-based or chat-based interaction.

In this work, the main effort was put towards creating a pipeline for fine-tuning all models for the specified \gls{NER} task.
This was due to the expectation that even small amounts of fine-tuning would be more effective in guiding model outputs than the best prompt could be.
% Thus, prompting was given only cursory attention.
More details on fine-tuning attempts in this work can be found in the later \secref{sft}.

\paragraph{Structured Output}
\texttt{guidance} \cite{guidance_2023}, originally from \gls{microsoft} research, allows more effective control over \glspl{LLM} than traditional prompting or chaining does.
In effect, \texttt{guidance} is a harness around a \gls{LLM}, providing support for the model in generating structured information, and making use of additional output structures such as CoT \cite{wei_chainofthought_2022} to result in higher-quality outputs.

For this work, specifically the library \texttt{jsonformer} \cite{1rgs_2023} is used, which does not provide the full feature suite of the \texttt{guidance} library.
At the time of deciding, \texttt{guidance} did not support models through the \gls{transformers} library yet.

The schema used for guidance can be seen in \coderef{schema}.
Additionally, the full prompt used can be seen in \coderef{prompt}, and an example output in \coderef{output}.


\code{schema.py}{schema}{The schema provided for the model to follow. Model output termination would happen after generation of a token for `\mintinline{python}{"}' for strings or `\texttt{,}' for numbers, or a number of other dedicated 'end of generation' tokens, e.g. \texttt{<EOS>}. See \coderef{output} for what an output for this schema might look like.}

\code{prompt.py}{prompt}{Prompt used to generate output. \mintinline{python}{"{output}"} delineates where the model provides an answer. See \coderef{output} for what may be filled in.}

\code{example_output.py}{output}{Exemplary output based on the prompt shown in \coderef{prompt}, and schema shown in \coderef{schema}.}


% \subsection{Prompt Engineering}\label{sub:engineering}
% Answers, even to the same prompts, across models and even from the same model, can vary substantially \cite{chen_how_2023}.
% Thus, a short-lived 'discipline', Prompt Engineering, emerged.
% Prompt Engineering attempted to find out how to write prompts to get the best results, out of either specific or all models.
% It was quickly found out that this is a task that can be automated with the help of a \gls{LLM} \cite{zhou_large_2022}.

% additional relevancy for applications where potentially hostile users can directly or indirectly prompt a model, and thus 'Prompt Injection Attacks' where born \cite{greshake_more_2023}.

% \subsection{Prompt Guidelines}\label{sub:guidelines}
% \todo{totally rewrite}
% A few general guidelines for prompts empirically emerged (mostly through people sharing results on twitter):
% \begin{itemize}
%     \item Guidance for everything structure-based \cite{guidance_2023}
%     \item Chain-Of-Thought for reasoning \cite{wei_chainofthought_2022}
%     \item Reflexion for even bigger models \cite{shinn_reflexion_2023}
% \end{itemize}

% \subsection{Prompts Used}\label{sub:prompts}


\section{Data Source}\label{sec:data}
As data source, 778 synthesis paragraphs and their corresponding labels from the publicly accessible database SynMOF\_M \cite{luo_mof_2022} where used.
Each synthesis paragraph describes the creation procedure of a \gls{MOF}.
This work focused on the basic parameters \ttemp, \ttime and \tsolv, though additional parameters could be added quickly to the schema discussed in the prior \secref{prompts}.

% TODO: maybe describe 'every synthesis has one
% or multiple temp, duration, solvent,
% optionally additive, and other parameters???

The labels from the SynMOF\_M database where manually annotated in prior work \cite{luo_mof_2022}.
% In total, those 905 labels and synthesis paragraphs where fully utilized during evaluation.
All proportional results in the later \chapref{results} are based on the accuracy of over all 778 items.
For training purposes, this dataset would have been split in dedicated datasets for test and training.
% A split in test and training dataset was performed for training, but this should not influence the accuracy for evaluation.
% Only a fraction of that would have been used for training purposes.
%, as fine-tuning with a few hundred examples is more than sufficient \cite{dunn_structured_2022}.

\section{Criteria for Equality}\label{sec:equality}
This section defines the criteria for determining equality between a result from a \gls{LLM} and the target label.
The criteria for \ttemp and \ttime and dealing with unit conversions is described in \subref{ttunit}.
Then, \subref{compsolv} detailes how compounds are compared.

\subsection{Time and Temperature}\label{sub:ttunit}
In the dataset (See \secref{data} for more details on the data source), all temperature information is encoded in degrees celsius, and all time information in hours.
Without a field for the unit (See \secref{prompts} for the prompts and structure used), models would use arbitrary units, often those used in the paragraph they are extracting from.
Since the task is not accurate unit conversion, but information extraction, a field for \ttemp and \ttime units was added.

Unit conversions for \ttemp and \ttime happen automatically before comparison and convert to a unified format, degrees celsius and hours respectively.
This ensures that durations of both '24h' and '1 day' are seen as equal, even though the strings are different.

\subsection{Compounds}\label{sub:compsolv}
Instead of names of chemical compounds, the database (See \secref{data} for more information on the data source) contains the \texttt{pubchempy}-\texttt{cid} (compound id) as the labels for solvents and additives (if applicable).
Most compounds have multiple different synonymous names they are known by, e.g. `water' has one \cid (which is 962) and a list of 319 distinct strings it can get resolved from.
Surprisingly, while the list of synonyms includes both `distilled water' and `H2O', it does not include `distilled H2O', which is mentioned verbatim in eight of the 778 synthesis paragraphs, and suggested as an answer by some models.
For more details on compound resolution problems, see \subref{solv}.

For each answer provided from the model for \tadd and \tsolv, an attempt at resolving the \cid is made.
If a \cid is found, it is compared with the label \cid.
An answer is counted as `wrong' when the resolved \cid is different to the label, but also when resolving fails.
Thus, the answer `distilled H2O' would be counted as `wrong', since it could not be resolved to a valid \cid.

% \subsection{F-Score}\label{sub:fscore}
% \draft{
% we don't attempt to assign categories to sections, instead extract the information directly
% }
% \todo{explain why fscore doesn't make much sense for us}

% For training purposes, the custom dataloader would search for any of the synonyms in the paragraph and use it as 'label'-text if found, or the first synonym if none could be easily identified.

