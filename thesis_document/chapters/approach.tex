\chapter{Approach}\label{chap:approach}
\todo{mention different modeling of NER task}
This chapter describes all relevant decisions of the approach taken, as well as their reasons.

First, \secref{impl} broadly describes the implementation, as well as libraries and frameworks used.
In \secref{models} descriptions and references to the various models considered for this work can be found.
\secref{prompts} discusses how the models where prompted in detail, and why little effort and experimentation was done with different prompts.
The dataset used, and how it was processed is described in \secref{data}, before it is depicted how the comparison for equality was done in \secref{equality}.


\section{Implementation}\label{sec:impl}
All source code for this work can be found at \url{https://github.com/fkarg/mthesis}, and a tag will mark the state at the time of submission.
\todo{publish on day of submission}

It became appearent during literature research that a comparison between different models would be valuable, and that additional models can be expected to be released in time to be included in this work.
Thus, almost all code ought to be model-agnostic.
The \acrlong{transformers} library is the perfect candidate for that: it is a well-established framework providing abstractions to load, manipulate and train any deep learning architecture in a standardized format.
Additionally, all open-access \glspl{LLM} are available directly through the \gls{hf} portal.

\paragraph{Dependencies}
Most other dependencies used are either straightforward (\texttt{torch}, \texttt{einops}, \texttt{accelerate}, \texttt{bitsandbytes} to get the models to run) or common ecosystem choices (e.g. \texttt{typer} and \texttt{rich} for the cli interface; \texttt{pubchempy} to resolve and convert chemical compounds; etc).

\paragraph{Modularity}
Proportionally speaking, the main module (with 36\%), dataloader (with 23\%), and unit conversion module (with 11\%) have the highest \gls{LOC} counts. Everything else is split in six more supporting modules.


\input{chapters/models}


\section{Prompts}\label{sec:prompts}
\glspl{LLM} are capable of very generic tasks, based on the input they are asked to respond to.
The way to get them to solve a task as requested is by \textit{prompting} the model with a certain input.
This is particularly eminent in models fine-tuned for instruction-based or chat-based interaction.

In this work, not put much effort was put in achieving the best prompts.
This was due to the expectation that even small amounts of fine-tuning would be more effective than the best prompt.
More details on fine-tuning attempts in this work can be found in the later \secref{sft}.

\informal{We} used \textit{guidance} \cite{guidance_2023}, and specifically the library \texttt{jsonformer} \cite{1rgs_2023}, for getting structured information as an output.
In effect, guidance provides `guard rails' for models generating output.
Specifically, the model does not have to generate the tokens for the structure of json, but only the tokens for the content of the json.

\code{schema.py}{schema}{The schema provided for the model to follow. Model output termination would happen after generation of a token for `\mintinline{python}{"}' for strings or `\texttt{,}' for numbers, or a number of other dedicated 'end of generation' tokens. See \coderef{output} for what an output for this schema might look like.}

\informal{You} can see the schema \informal{we} used for guidance in \coderef{schema}. Additionally, you can find the full prompt used in \coderef{prompt} and an example output in \coderef{output}.

\code{prompt.py}{prompt}{Prompt used to generate output. \mintinline{python}{"{output}"} delineates where the model provides an answer. See \coderef{output} for what may be filled in.}

\code{example_output.py}{output}{Exemplary output based on the prompt shown in \coderef{prompt}.}


% \subsection{Prompt Engineering}\label{sub:engineering}
% Answers, even to the same prompts, across models and even from the same model, can vary substantially \cite{chen_how_2023}.
% Thus, a short-lived 'discipline', Prompt Engineering, emerged.
% Prompt Engineering attempted to find out how to write prompts to get the best results, out of either specific or all models.
% It was quickly found out that this is a task that can be automated with the help of a \gls{LLM} \cite{zhou_large_2022}.

% additional relevancy for applications where potentially hostile users can directly or indirectly prompt a model, and thus 'Prompt Injection Attacks' where born \cite{greshake_more_2023}.

% \subsection{Prompt Guidelines}\label{sub:guidelines}
% \todo{totally rewrite}
% A few general guidelines for prompts empirically emerged (mostly through people sharing results on twitter):
% \begin{itemize}
%     \item Guidance for everything structure-based \cite{guidance_2023}
%     \item Chain-Of-Thought for reasoning \cite{wei_chainofthought_2022}
%     \item Reflexion for even bigger models \cite{shinn_reflexion_2023}
% \end{itemize}

% \subsection{Prompts Used}\label{sub:prompts}


\section{Data Source}\label{sec:data}
\informal{We} used 905 synthesis paragraphs which were used to create parts of the publicly accessible labels in the databases SynMOF\_A and SynMOF\_M \cite{luo_mof_2022}.
\informal{We} defaulted to get a label from SynMOF\_M (manually annotated) if it was available, and manually confirmed the validity of the SynMOF\_A label (generated from automatic extraction) if it existed.
\todo{check again if default was A or M}
In total, \informal{we} had labels for 905 synthesis paragraphs which \informal{we} fully utilized.
All proportional results in the later \chapref{results} are based on the accuracy of over 905 items.

\section{Criteria for Equality}\label{sec:equality}
In this section \informal{we} list the criteria used for equality between a result from a \gls{LLM} and the target label.
\draft{
First, the criteria for \ttemp and \ttime and dealing with unit conversions is described in \subref{ttunit}.
Second, \subref{compsolv} describes how compounds are compared.
}

\subsection{Time and Temperature}\label{sub:ttunit}
In \informal{our} dataset (See \secref{data} for more details on the data source), all temperature information was encoded in degrees celsius, and all time information in hours.
Without a field for the unit (See \secref{prompts} for the prompts and structure used), models would use arbitrary units, often those used in the paragraph they are extracting from.
Since the task is not accurate unit conversion, but simply information extraction, \informal{we} added fields for units.

Unit conversions for \ttemp and \ttime happen automatically and convert to a unified format. This ensures that durations of both '24h' and '1 day' are seen as equal.

\subsection{Compounds}\label{sub:compsolv}
Instead of names from chemical compounds, \informal{our} database (See \secref{data} for more information on the data source) has the \texttt{pubchempy}-\texttt{cid} (compound id) as label for solvents and additives (if used).
Most compounds have multiple different synonyms they are known by, e.g. `water' has one \texttt{cid} and 319 distinct synonyms it can get resolved from.

Thus, it is tried to resolve the answer from the model to a \texttt{cid}, and compare it with the given label.
For training purposes, the custom dataloader would search for any of the synonyms in the paragraph and use it as 'label'-text if found, or the first synonym if none could be easily identified.

% (sometimes adding too many or too few zeros, though also often getting it right)
