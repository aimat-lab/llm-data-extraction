\chapter{Background}\label{chap:background}
\todo{write overview of background}
% Evaluation criteria:
% Completeness (breadth)
% Level of detail (depth)
% Quality


% \section{Building a Database}\label{sec:database}
% The overarching goal of any Information Extraction task in the natural sciences is to build a database 
% 
% Such databases are often built using \texttt{chemdataextractor} \cite{swain_chemdataextractor_2016}.



\section{Related Work}\label{sec:related}
\subsection{Information Extraction}\label{sub:extraction}
Information Extraction is the \gls{NLP} task of extracting structured (machine-readable) information from unstructured text.
One approach to information extraction is via \gls{NER}.

\subsection{Named Entity Recognition}\label{sub:NER}
\gls{NER} seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories \cite{li_survey_2022} such as temperature, timeframe, or chemical components, or any other named category that has been specified beforehand.
Fundamentally speaking, this is what we are doing in our work.
The usual \gls{NER} definition of an entity assumes \textit{rigid designators} \cite{laporte_rigid_2022}, i.e. that an entity only has one name to reference it.

In our work, we are however expanding this usual \gls{NER} definition on rigid designators.
First, references to temperature can include phrases such as `at room temperature' or `in boiling water', and may not just be numbers followed by their unit.
Second, while Chemistry has normative rules on how molecules are named, it is possible for one molecule to have multiple valid names. This is sometimes referred to as the \textit{coreference problem} \cite{hobbs_coherence_1979}.

% It is then possible to automatically construct tabular databases based on the extracted data \cite{zhao_finetuning_2021} (BERT-SFT for matsci).


\subsection{Rule-Based Entity Recognition}
There have long been rule-based approaches for the recognition of named entities (e.g. Temperature) for material science literature.
ChemTagger \cite{hawizy_chemicaltagger_2011}, and others \cite{beard_comparative_2019, huang_database_2020}
clearly demonstrated that systems based on regular expressions can accurately extract information in straightforward, well-defined situations.
% They can often achieve high precision for simple, well-defined tasks
However, these tools tend to be highly specialized which makes them hard to adapt to new or more complex queries or circumstances.


\subsection{Language Models for Information Extraction}
Modeling \gls{NER} as a sequence-to-sequence labeling problem, \cite{zhao_finetuning_2021} fine-tuned a pretrained \gls{BERT} instance and achieved an \gls{fscore} of 85\%, demonstrating both high precision and recall (where the \gls{fscore} is \glsdesc{fscore}).
This demonstrated the fundamental capability of \glspl{LM} for this task.
Most other work on \gls{NER} using \glspl{LM} has not been on materials science text but on general purpose text \cite{li_survey_2022}.

Our work fundamentally differs, as we use \glspl{causal} wheras \gls{BERT} is a \gls{masked}. See \secref{masked} for an outlook on comparison with modern \gls{masked} architectures. See the later \secref{basics} for more details on \acrlong{LM}.

\subsection{LLMs for Information Extraction}
\todo{write subsection on LLMs for IE}
{\color{blue}
Our currently most capable models for \gls{NLP} (?) are \glspl{LLM}. See the later \subref{llm} for a more extensive introduction on \glspl{LLM}. 

Using a \gls{LLM} such as \gls{GPT3} for a mixed task of \gls{NER} and relation extraction ended with very mixed results for \gls{NER}. \cite{dunn_structured_2022}
We're doing \gls{NER} only.
}

% Chinchilla \cite{hoffmann_training_2022} and CoTR \cite{zhang_multimodal_2023}
% demonstrated that while achieving impressive capability, such large models are
% substantially overparametrized and undertrained.

\section{Scientific Question}\label{sec:question}

The goals of this work are threefold
\begin{enumerate}
    \item Demonstrate zero-shot automated information extraction from scientific literature using open-access \glspl{LLM}.
    \item Benchmark\todo{Benchmark or Compare?} the accuracy of currently available open-access \glspl{LLM} for automated information extraction from scientific literature.
    \item Attempt fine-tuning of open-access \glspl{LLM} in order to increase accuracy.
\end{enumerate}

As part of this work, we will create a flexible automated pipeline for the extraction of non-machine readable information in \gls{MOF} synthesis.
Refer to \chapref{approach} for more details on our approach, specifically \secref{impl} for more details on our implementation, \secref{models} for more details on the models used, and \secref{data} for more details on the data source.
