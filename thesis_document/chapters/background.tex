\chapter{Background}\label{chap:background}
% Evaluation criteria:
% Completeness (breadth)
% Level of detail (depth)
% Quality

This chapter gives a short introduction to the parts of \acrlong{NLP} necessary for this work, and highlights other relevant work.

First, \subref{extraction} broadly defines the \gls{NLP} task of Information Extraction.
\subref{NER} extends this by defining \gls{NER} and how this work expands upon the usual definition of \gls{NER}.
Then, \subref{rule-based} highlights early approaches and solutions, \subref{lmie} explains how \glspl{LM} are being used for such tasks, and finally \subref{llmie} references the usages of \glspl{LLM} for this use-case.



% \section{Building a Database}\label{sec:database}

\section{Natural Language Processing}\label{sec:NLP}
\gls{NLP} is an interdisciplinary field of study between computer science and linguistics, and is primarily concerned with giving computers the ability to ``understand'' the contents of documents.
This often requires contextual nuances to accurately extract information.
For this work, primarily the task of Information Extraction explained in the next \subref{extraction} and specifically the subtask \gls{NER} explained in following \subref{NER} are of importance.

\subsection{Information Extraction}\label{sub:extraction}
Information Extraction is the \gls{NLP} task of extracting structured (machine-readable) information from unstructured text.
In most cases the extended goal of information extraction in the natural sciences is to create a database of the extracted information.
Such databases are then used to train further models used to predict attributes or synthesis conditions \cite{luo_mof_2022}, or to substantially improve automated experimentation \cite{shi_automated_2021}.
In this work, no such database will be created, as the goal is instead to benchmark the accuracy of various models for such a task.
One approach to information extraction is via \gls{NER}.
% Such databases are often built using \texttt{chemdataextractor} \cite{swain_chemdataextractor_2016}.

\subsection{Named Entity Recognition}\label{sub:NER}
\gls{NER} seeks to locate and classify named entities (e.g. classify `water' as solvent) mentioned in unstructured text into pre-defined categories \cite{li_survey_2022} such as temperature, timeframe, or solvent.

The usual \gls{NER} definition of an entity assumes \textit{rigid designators} \cite{laporte_rigid_2022}, i.e. that an entity only has one name to reference it.
This work uses the usual definition of \gls{NER}, but loosens the expectation of rigid designators in the following two ways:

\begin{itemize}
    \item First, references to temperature can include phrases such as `at room temperature' or `in boiling water', and may not just be numbers followed by their unit.
    \item Second, while Chemistry has normative rules on how molecules are named, it is possible for one molecule to have multiple valid names.
In \gls{NLP}, this is sometimes referred to as the \textit{coreference problem} \cite{hobbs_coherence_1979}.
\end{itemize}



\section{Related Work}\label{sec:related}

\subsection{Rule-Based Entity Recognition}\label{sub:rule-based}
There have long been rule-based approaches for the recognition of named entities (e.g. temperature) for material science literature.
ChemTagger \cite{hawizy_chemicaltagger_2011}, and others \cite{beard_comparative_2019, huang_database_2020}
clearly demonstrated that systems based on regular expressions can accurately extract information in straightforward, well-defined situations.
% They can often achieve high precision for simple, well-defined tasks
However, these tools tend to be highly specialized which makes them hard to adapt to new or more complex queries or circumstances.


\subsection{Language Models for Information Extraction}\label{sub:lmie}
\gls{NER} is sometimes modelled as a sequence-to-sequence labeling problem \cite{zhao_finetuning_2021, dunn_structured_2022}.
Zhao et al. fine-tuned a number of pretrained \gls{BERT} instances and achieved an \gls{fscore} of 85\% \cite{zhao_finetuning_2021}, demonstrating both high precision and recall (and \gls{fscore} is \glsdesc{fscore}).
This demonstrated the fundamental capability of \glspl{LM} for this task.
Most other work on \gls{NER} using \glspl{LM} has not been on materials science literature but on general purpose text \cite{li_survey_2022}.


In this work, \gls{NER} is not modeled as a sequence-to-sequence labeling problem, which is why \gls{fscore} is not an applicable metric for performance.
Additionally, a distinction has to be made between a \gls{masked} such as \gls{BERT} and all well-known \glspl{LLM} which are \glspl{causal}.
A \gls{causal} predicts the likelihood of the next token (piece of text) based on a previous sequence of tokens, often called \textit{input}, \textit{prompt} or \textit{context}.
In contrast, a \gls{masked} predicts attributes of, or tokens in potentially multiple masked locations as part of a sequence of tokens, taking the full context of all surrounding tokens into account.
The output of a \gls{masked} are properties or tokens for each masked location, whereas the outupt of a \gls{causal} is a likelihood distribution for the next token in the sequence. 
% Additionally, in this work \glspl{causal} are used, whereas \gls{BERT} is a \gls{masked}.
\secref{masked} outlines how a comparison with modern \gls{masked} architectures might look like. Additionally, \secref{basics} describes \acrlongpl{LM} in more detail.

\subsection{LLMs for Information Extraction}\label{sub:llmie}
\glspl{LLM}, usually in the form of a \gls{causal}, are a recent phenomenon, and at the forefront of \gls{NLP} research.
There is not much prior work on \gls{NER} using a \gls{LLM}.
A combined \gls{NER} and Relation Extraction approach using \gls{GPT3} has been tried with some success \cite{dunn_structured_2022}.
The primary focus of Dunn et al. \cite{dunn_structured_2022} is on relation extraction, which is why \glspl{fscore} for the \gls{NER} task vary between 0.4 and 0.95.
This work differs on two counts from \cite{dunn_structured_2022}: 1) a benchmark and comparison of multiple open-access \glspl{LLM} is done and 2) the entire focus of this work is on the extended \gls{NER} task.

A more extensive introduction to \glspl{LLM} is given in the later \subref{llm}.

% Chinchilla \cite{hoffmann_training_2022} and CoTR \cite{zhang_multimodal_2023}
% demonstrated that while achieving impressive capability, such large models are
% substantially overparametrized and undertrained.
