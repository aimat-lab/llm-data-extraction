\chapter{Conclusion}\label{chap:conclusion}
% \todo{conclusion chapter guideline: what was learned from results based on initial question from the experiments and from other sources?}
% it turned out that the models where good in extracting existing information
% but they could not detect if information was present at all (though maybe due to prompting), or it was simply not mapped as classification.

Coming back to the initially posed goals of this work from \secref{question}:

First, zero-shot automated information extraction from scientific literature was successfully demonstrated with high accuracy achieved by most models of all sizes, as detailed in \secref{result:first}.

Second, the capabilities of some of the most capable open-access \glspl{LLM} was measured and compared specifically for the information extraction task on scientific data, detailed also in \secref{result:first}.
Furthermore, analyzing frequent mistakes in \secref{mistakes} gave additional insight in failure modes and further venues for inquiry and improvement.

Third, a lot of time and effort was put in attempting to fine-tune models for the information extraction task, which was substantially harder than initially assumed, and eventually abandoned.
\secref{res:sft} has excerpts of the difficulties encountered, which gives some insight to the task of fine-tuning in practice.


This work demonstrates and measures the fundamental capability of various models for information extraction tasks for scientific literature in a zero-shot setting with an easily configurable pipeline.
This pipeline can easily be configured to extract additional parameters.
Hybrid systems (automatic extraction with manual oversight) could substantially increase data quality for various domains of research even for very complex parameters.
This indicates that \glspl{LLM} can be powerful tools for information extraction, even in setups where the model is self-hosted with no fine-tuning.

Most surprising is the comparatively high accuracy that 13B-sized models are able to achieve, particularly \model{llama} and \model{llama2}.
Many consumer graphics cards have VRAM of more than 10GB available, which is sufficient to load 13B-sized models with 4-bit quantization and achieve a throughput of 30 to 40 tokens per second, depending on the specific GPU used \cite{hardwarerequirements_2023}.
Thus, high-accuracy information extraction capabilities are much more accessible than previously expected, which could have far-reaching implications for the future of \gls{NLP}.







