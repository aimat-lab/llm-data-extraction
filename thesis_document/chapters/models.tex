\section{Language Models Considered}\label{sec:models}
The following sections describe the various models considered for the benchmark, and the reasons for or against their inclusion.
Before that, \subref{criteria} depicts simple criteria for model selection.

See \secref{basics} for an overview and broad categorization of the models mentioned here.


\subsection{Criteria}\label{sub:criteria}
For this work, the following criteria where chosen:
\begin{enumerate}
    \item It is possible to get the full model weights.
    \item The selected models ought to be decently capable \gls{causal}.
    \item Ceteris paribus, a smaller model is better.
\end{enumerate}

First, the model weights need to be available. This is necessary to run the model in a self-hosted manner.
Practically, this is a constraint towards open-source or at least open-access models (their license needs to allow for academic research).
\todo{reason for criteria: people want to fine-tune models on their own potentially sensitive data}

Second, at this point dozens of \glspl{LLM} have been trained, and weights made available.
One model was trained purely as a marketing pitch to sell computing systems \cite{dey_cerebrasgpt_2023}.
Thus, models were selected partially based on proven capability in other domains.

Third, all else being equal a smaller model is preferred.
This is due smaller models being less resource intensive to evaluate and fine-tune.
Having a smaller variant of a model is a nice bonus which enables faster iteration during development.


\subsection{OPT, BLOOM (Not Used)}
\paragraph{OPT}\label{par:opt}\label{sub:OPT}
The initial model set out for this work was \model{OPT} \cite{zhang_opt_2022}, a 175 billion parameter open-source \gls{LLM} trained by \gls{meta}, with partially similar capability as \gls{GPT3}. During early literature research, we encountered the similar but slightly more capable \model{BLOOM}.

\paragraph{BLOOM}\label{par:bloom}\label{sub:BLOOM}
\model{BLOOM} \cite{workshop_bloom_2022} is a 176 billion parameter open-source \gls{LLM} trained by a cooperation of numerous organizations, spearheaded by \gls{hf} and \gls{Google}. When compared to \model{OPT} across \gls{NLP} benchmarks, \model{BLOOM} appears to perform marginally better.

\paragraph{Reasons for Using Neither}
The original plan for this work would use \model{OPT} as only model. During early literature research, it seemed that \model{BLOOM} would be slightly more capable, which changed the intention to compare both.
Not soon after, the smaller and seemingly much more capable \model{llama} was released, which prompted the decision of creating a model-agnostic pipeline instead, focusing on \model{llama} first.
See the next \subref{llama} for more details on \model{llama}.

\subsection{LLaMa (Used)}\label{sub:llama}
\model{llama} is a family of open-access \glspl{LLM} from \gls{meta} with sizes ranging from 7 billion to 65 billion parameters, and capabilities comparable to, and sometimes beating \gls{SOTA} (including the substantially larger \gls{GPT3}) at the time of release \cite{touvron_llama_2023}.
\model{llama} can be seen as the first culmination of progress on \glspl{LLM} in one place.

\model{llama} is not instruction fine-tuned. See \subref{instruct} for more details on instruction fine-tuning.
For instruction fine-tuned variants of \model{llama}, see \subref{alpaca} on \model{alpaca} or \subref{vicuna} on \model{vicuna}.

\subsection{Alpaca (Not Used)}\label{sub:alpaca}
The \model{alpaca} Project \cite{tatsulab_2023} aims to build and share an instruction fine-tuned \model{llama} model.
Due to uncertainty with the \model{llama} licence which this model is based on (it was fully released a mere two weeks after \gls{llama} was first announced), no model weights where released officially.
Instead, all scripts and training data to fine-tune your own \model{alpaca} based on existing \model{llama} weights where provided. 
Fine-tuning on a large dataset becomes impractical for larger model variants due to rapidly increasing resource requirements.
For this reason, it was decided against including \model{alpaca} in the benchmark.


\subsection{Vicuna (Used Partially)}\label{sub:vicuna}
\model{vicuna} is a family of instruction fine-tuned \model{llama}-variants, released by \gls{lmsys}. It is building on top of the training recipe of \model{alpaca}.
However, not all weights of the corresponding \model{llama} sizes are available.
The largest \model{llama}-model (65B) does not have a corresponding \model{vicuna} derivative available.
In a tournament format between different \glspl{LLM}, \model{vicuna} provided user-preferred answers more often than \model{llama} and \model{alpaca} \cite{zheng_judging_2023}, among others.
Thus, before the release of \model{falcon} and \model{llama2}, \model{vicuna} was generally seen as the most capable instruct-based model, which is why it was included in this benchmark.


\subsection{LLaMa 2 (Used)}\label{sub:llama2}
\gls{meta} released \model{llama2} \cite{touvron_llama2_2023} only a few months after the release of its predecessor.
They introduced few fundamental changes when compared to \model{llama}.
The main differences include making use of \gls{GQA} for the first time, and training on more tokens.
For each size, \model{llama2} was released in four versions: 1) the base model 2) a 'helpful'-variant trained with human-feedback 3) a 'chat' variant optimized for dialogue and 4) a combined 'helpful' and 'chat' variant.


\subsection{Falcon (Used)}\label{sub:falcon}
The \model{falcon} \cite{zxhang_falcon_2023} family of language models are created by the Abu Dhabi-based \gls{tii}.
Since its release, \model{falcon} is to be at the top of most benchmarks between open-access models (in each respective parameter size category) \cite{zxhang_falcon_2023}.
It appears to rival some of the most capable closed-access models such as \gls{PaLM} in capability.

Their better performance for most tasks is assumed to mostly be the result of longer training and higher-quality data sets \cite{zxhang_falcon_2023}.

Recently, a new 180 billion parameter \model{falcon} variant was announced \cite{tii_falcon180b_2023}. \model{falcon}-180B is not included in this benchmark.


\subsection{GPT4 (Not Used)}\label{sub:GPT4}
\model{GPT4} is the fourth generation \gls{GPT} model from \gls{OpenAI} \cite{openai_gpt4_2023}.
It is the single most capable \acrlong{LM} we currently know of.
However, it is not open-source and only accessible through interfaces provided by \gls{OpenAI}.
Additionally, \gls{OpenAI} continues to work on, change, and sometimes degrade the capabalities of \model{GPT4} \cite{chen_how_2023}.
Even timestamp-versioned, 'unchanging' models have been claimed to measurably change in behaviour \cite{jw1224_hn}.
This makes it a hard target for comparison.

\model{GPT4} does not fulfill the criteria of being open-access, and is thus not compared in this work.


\subsection{Final List}\label{sub:list}
In conclusion, we used the following models and sizes of the aforementioned:
\begin{itemize}
    \item \model{llama} 7B, 13B, 30B, 65B (See \subref{llama} for more details on the model)
    \item \model{vicuna} 7B, 13B, 33B (See \subref{vicuna} for more details on the model)
    \item \model{llama2} 7B, 13B, 70B (See \subref{llama2} for more details on the model)
    \item \model{falcon} 7B, 40B (See \subref{falcon} for more details on the model)
    \item \model{falcon}-instruct 7B, 40B (See \subref{falcon} for more details on the model)
\end{itemize}
