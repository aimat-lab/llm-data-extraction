\begin{figure}[!htb]
    \begin{centering}
        \includegraphics[height=0.4\textheight]{img/modern_transformer}
        \caption[Example of a Modern Transformer Architecture]{\textbf{Example of a Modern Transformer Architecture.}
        There are a number of differences when compared to the original architecture as seen previously in \figref{transformer}:
    layers are normalizing the residual with RMSNorm instead of having a normalized residual.
Multi-head attention was replaced with \gls{GQA}, and sinusoidal positional embeddings with embeddings from \gls{RoPE} {\em on each layer}. Additionally, activation functions in the \gls{MLP} changed from \gls{ReLU} to \gls{SwiGLU}.}
        \label{fig:modern_transformer}
    \end{centering}
\end{figure}

